{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cynric1/smarthusky1/blob/main/nested_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# It is the work in term 1, including defining a tree, calculating the nest distance between trees and generating random trees.\n",
        "class TreeNode:\n",
        "    def __init__(self, layer, ancestor=None, transition_prob=None, value=None):\n",
        "        self.layer = layer\n",
        "        self.ancestor = ancestor\n",
        "        self.transition_prob = transition_prob\n",
        "        self.value = value\n",
        "        self.descendants = []\n",
        "\n",
        "    def add_descendant(self, node):\n",
        "        self.descendants.append(node)\n",
        "\n",
        "    def __repr__(self):\n",
        "        ancestor_value = self.ancestor.value if self.ancestor else None\n",
        "        return (f\"TreeNode(layer={self.layer}, value={self.value}, \"\n",
        "                f\"ancestor_value={ancestor_value}, \"\n",
        "                f\"transition_prob={self.transition_prob})\")\n",
        "\n",
        "class Tree:\n",
        "    def __init__(self):\n",
        "        self.nodes = []\n",
        "\n",
        "    def add_node(self, node):\n",
        "        self.nodes.append(node)\n",
        "\n",
        "    def get_nodes_at_layer(self, layer):\n",
        "        return [node for node in self.nodes if node.layer == layer]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Tree with {len(self.nodes)} nodes:\\n\" + \"\\n\".join([str(node) for node in self.nodes])\n",
        "\n",
        "def compute_wasserstein_distance_with_cost(cost_matrix, p, max_iter=1000, tol=1e-12):\n",
        "    m, n = cost_matrix.shape\n",
        "    cost_matrix_p = cost_matrix ** p\n",
        "    K = np.exp(-cost_matrix_p)\n",
        "    u = np.ones(m)\n",
        "    v = np.ones(n)\n",
        "    p = np.ones(m) / m\n",
        "    q = np.ones(n) / n\n",
        "    max_iter = int(max_iter)\n",
        "\n",
        "    for iteration in range(max_iter):\n",
        "        u_prev, v_prev = u.copy(), v.copy()\n",
        "        u = p / (K @ v)\n",
        "        v = q / (K.T @ u)\n",
        "        if np.linalg.norm(u - u_prev) < tol and np.linalg.norm(v - v_prev) < tol:\n",
        "            break\n",
        "\n",
        "    P = np.diag(u) @ K @ np.diag(v)\n",
        "    distance = np.sum(P * cost_matrix_p)\n",
        "    return distance, P\n",
        "\n",
        "def calculate_local_distance(node_a, node_b, p):\n",
        "    \"\"\"Calculate the local distance between two nodes using canonical distance to the power of p.\"\"\"\n",
        "    return abs(node_a.value - node_b.value) ** p\n",
        "\n",
        "def generate_random_tree(num_layers=10, max_children=2):\n",
        "    tree = Tree()\n",
        "    root = TreeNode(layer=0, value=random.uniform(0.99, 1.01)) # Here, I try to mimic the real daily return, and assume daily increases and decrease <1%\n",
        "    tree.add_node(root)\n",
        "    nodes_by_layer = {0: [root]}\n",
        "\n",
        "    for layer in range(1, num_layers + 1):\n",
        "        nodes_by_layer[layer] = []\n",
        "        for ancestor in nodes_by_layer[layer - 1]:\n",
        "            num_children = random.randint(1, max_children)\n",
        "            # Normalize transition probabilities\n",
        "            transition_probs = [random.uniform(0, 1) for _ in range(num_children)]\n",
        "            total_prob = sum(transition_probs)\n",
        "            transition_probs = [p/total_prob for p in transition_probs]\n",
        "\n",
        "            for i in range(num_children):\n",
        "                child_value = random.uniform(0.99, 1.01) # Similarly as before for each node\n",
        "                transition_prob = transition_probs[i]\n",
        "                child = TreeNode(layer=layer, ancestor=ancestor, transition_prob=transition_prob, value=child_value)\n",
        "                ancestor.add_descendant(child)\n",
        "                tree.add_node(child)\n",
        "                nodes_by_layer[layer].append(child)\n",
        "\n",
        "    return tree\n",
        "\n",
        "def compute_nested_wasserstein_distance(tree_a, tree_b, p=1, max_iter=1000, tol=1e-12):\n",
        "    \"\"\"\n",
        "    Compute the nested Wasserstein distance between two trees, storing coupling matrices for node pairs.\n",
        "    Args:\n",
        "        tree_a (Tree): The first tree.\n",
        "        tree_b (Tree): The second tree.\n",
        "        p (int): The power to which the canonical distance is raised.\n",
        "        max_iter (int): Maximum iterations for Sinkhorn's algorithm.\n",
        "        tol (float): Convergence tolerance for Sinkhorn's algorithm.\n",
        "    Returns:\n",
        "        total_distance (float): The nested Wasserstein distance between the two trees.\n",
        "        node_distances (dict): Wasserstein distances for each node pair.\n",
        "        node_coupling_matrices (dict): Coupling matrices for each node pair.\n",
        "    \"\"\"\n",
        "    node_distances = {}\n",
        "    node_coupling_matrices = {}\n",
        "\n",
        "    max_layer_a = max([node.layer for node in tree_a.nodes]) if tree_a.nodes else 0\n",
        "    max_layer_b = max([node.layer for node in tree_b.nodes]) if tree_b.nodes else 0\n",
        "    layer = max(max_layer_a, max_layer_b)\n",
        "\n",
        "    # Leaf layer\n",
        "    leaf_nodes_a = tree_a.get_nodes_at_layer(layer)\n",
        "    leaf_nodes_b = tree_b.get_nodes_at_layer(layer)\n",
        "\n",
        "    if not leaf_nodes_a or not leaf_nodes_b:\n",
        "        return float('inf'), node_distances, node_coupling_matrices\n",
        "\n",
        "    cost_matrix = np.zeros((len(leaf_nodes_a), len(leaf_nodes_b)))\n",
        "    for i, node_a in enumerate(leaf_nodes_a):\n",
        "        for j, node_b in enumerate(leaf_nodes_b):\n",
        "            cost_matrix[i, j] = calculate_local_distance(node_a, node_b, p)\n",
        "\n",
        "    distance, coupling = compute_wasserstein_distance_with_cost(cost_matrix, p, max_iter, tol)\n",
        "\n",
        "    # Store distance and coupling for each pair of leaf nodes\n",
        "    for i in range(len(leaf_nodes_a)):\n",
        "        for j in range(len(leaf_nodes_b)):\n",
        "            node_distances[(layer, i, j)] = distance\n",
        "            node_coupling_matrices[(layer, i, j)] = coupling\n",
        "\n",
        "    # Backwards\n",
        "    for layer in range(layer - 1, -1, -1):\n",
        "        nodes_a = tree_a.get_nodes_at_layer(layer)\n",
        "        nodes_b = tree_b.get_nodes_at_layer(layer)\n",
        "\n",
        "        for i, node_a in enumerate(nodes_a):\n",
        "            for j, node_b in enumerate(nodes_b):\n",
        "                descendants_a = node_a.descendants\n",
        "                descendants_b = node_b.descendants\n",
        "\n",
        "                if not descendants_a or not descendants_b:\n",
        "                    node_distances[(layer, i, j)] = calculate_local_distance(node_a, node_b, p)\n",
        "                    node_coupling_matrices[(layer, i, j)] = np.array([[1]])\n",
        "                    continue\n",
        "\n",
        "                descendant_cost_matrix = np.zeros((len(descendants_a), len(descendants_b)))\n",
        "\n",
        "                for k, descendant_a in enumerate(descendants_a):\n",
        "                    descendant_layer_a = descendant_a.layer\n",
        "                    descendant_idx_a = tree_a.get_nodes_at_layer(descendant_layer_a).index(descendant_a)\n",
        "\n",
        "                    for l, descendant_b in enumerate(descendants_b):\n",
        "                        descendant_layer_b = descendant_b.layer\n",
        "                        descendant_idx_b = tree_b.get_nodes_at_layer(descendant_layer_b).index(descendant_b)\n",
        "\n",
        "                        # Distance between descendants\n",
        "                        key = (descendant_layer_a, descendant_idx_a, descendant_idx_b)\n",
        "                        if key in node_distances:\n",
        "                            descendant_cost_matrix[k, l] = node_distances[key]\n",
        "                        else:\n",
        "                            # If distance not computed, use local distance as fallback\n",
        "                            descendant_cost_matrix[k, l] = calculate_local_distance(descendant_a, descendant_b, p)\n",
        "\n",
        "                # Compute Wasserstein distance and coupling matrix for the current node pair\n",
        "                descendant_distance, descendant_coupling_matrix = compute_wasserstein_distance_with_cost(\n",
        "                    descendant_cost_matrix, p, max_iter, tol)\n",
        "\n",
        "                node_distances[(layer, i, j)] = descendant_distance + calculate_local_distance(node_a, node_b, p)\n",
        "                node_coupling_matrices[(layer, i, j)] = descendant_coupling_matrix\n",
        "\n",
        "    if (0, 0, 0) in node_distances:\n",
        "        root_distance = node_distances[(0, 0, 0)]\n",
        "    else:\n",
        "        # Fallback if root distance not calculated\n",
        "        root_distance = float('inf')\n",
        "\n",
        "    return root_distance, node_distances, node_coupling_matrices\n",
        "\n",
        "def sample_path_from_tree(tree, num_layers=None):\n",
        "    \"\"\"Sample a path from a tree based on transition probabilities.\"\"\"\n",
        "    if not tree.nodes:\n",
        "        return []\n",
        "\n",
        "    # Find the root node\n",
        "    root = None\n",
        "    for node in tree.nodes:\n",
        "        if node.layer == 0:\n",
        "            root = node\n",
        "            break\n",
        "\n",
        "    if not root:\n",
        "        return []\n",
        "\n",
        "    # Initialize the path with the root node\n",
        "    path = [root]\n",
        "    current_node = root\n",
        "\n",
        "    # Sample nodes for each subsequent layer\n",
        "    for layer in range(1, num_layers):\n",
        "        descendants = current_node.descendants\n",
        "\n",
        "        if not descendants:\n",
        "            break\n",
        "\n",
        "        # Get transition probabilities and sample next node\n",
        "        probs = [node.transition_prob for node in descendants]\n",
        "        next_node = random.choices(descendants, weights=probs, k=1)[0]\n",
        "        path.append(next_node)\n",
        "        current_node = next_node\n",
        "\n",
        "    return path\n",
        "\n",
        "def path_to_values_vector(path):\n",
        "    \"\"\"Convert a path of TreeNodes to values.\"\"\"\n",
        "    expected_length = 10 # 10 just in this example\n",
        "    values = [node.value for node in path]\n",
        "    return np.array(values)\n",
        "\n",
        "def path_to_binary_label(path):\n",
        "    \"\"\"\n",
        "    Convert a path to binary labels (1 or -1) based on node values.\n",
        "    If node value > 1, then 1 (price increase), otherwise -1 (price decrease or flat).\n",
        "    \"\"\"\n",
        "    expected_length = 10\n",
        "    labels = [1 if node.value > 1 else -1 for node in path]\n",
        "    return np.array(labels)\n",
        "\n",
        "def create_neural_network(input_size=10):\n",
        "\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(input_size,)),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dense(input_size, activation='tanh')\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def calculate_accuracy(predictions, ground_truth):\n",
        "    \"\"\"\n",
        "    Calculate accuracy between binary predictions and ground truth.\n",
        "\n",
        "    Args:\n",
        "        predictions (numpy.ndarray): Predicted values.\n",
        "        ground_truth (numpy.ndarray): Ground truth values.\n",
        "\n",
        "    Returns:\n",
        "        float: Accuracy as a percentage.\n",
        "    \"\"\"\n",
        "    # Convert predictions to binary classes\n",
        "    binary_predictions = np.where(predictions > 0, 1, -1)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    correct = np.sum(binary_predictions == ground_truth)\n",
        "    total = len(ground_truth)\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "def generate_and_filter_trees(num_nu_trees=200, num_layers=10, max_children=2, distance_threshold=0.05):\n",
        "    \"\"\"\n",
        "    Generate a reference tree (mu) and multiple comparison trees (nu_1 to nu_n),\n",
        "    then filter based on distance threshold.\n",
        "\n",
        "    Args:\n",
        "        num_nu_trees (int): Number of nu trees to generate.\n",
        "        num_layers (int): Number of layers in each tree.\n",
        "        max_children (int): Maximum number of children per node.\n",
        "        distance_threshold (float): Maximum distance for filtering.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (mu_tree, mu_path, filtered_nu_trees, filtered_nu_paths, filtered_distances)\n",
        "    \"\"\"\n",
        "    # Generate reference tree (mu)\n",
        "    mu_tree = generate_random_tree(num_layers, max_children)\n",
        "\n",
        "    # Sample a single path from mu\n",
        "    mu_path = sample_path_from_tree(mu_tree, num_layers)\n",
        "\n",
        "    # Generate comparison trees (nu_1 to nu_n)\n",
        "    nu_trees = []\n",
        "    nu_paths = []\n",
        "    distances = []\n",
        "\n",
        "    print(f\"Generating and filtering {num_nu_trees} nu trees...\")\n",
        "\n",
        "    # Generate nu trees and compute distances\n",
        "    for i in range(num_nu_trees):\n",
        "        nu_tree = generate_random_tree(num_layers, max_children)\n",
        "        nu_path = sample_path_from_tree(nu_tree, num_layers)\n",
        "\n",
        "        # Compute distance between mu and nu\n",
        "        distance, _, _ = compute_nested_wasserstein_distance(mu_tree, nu_tree)\n",
        "\n",
        "        if distance < distance_threshold:\n",
        "            nu_trees.append(nu_tree)\n",
        "            nu_paths.append(nu_path)\n",
        "            distances.append(distance)\n",
        "\n",
        "        if (i+1) % 20 == 0:\n",
        "            print(f\"Processed {i+1}/{num_nu_trees} trees, found {len(nu_trees)} within threshold\")\n",
        "\n",
        "    # Convert distances to numpy array\n",
        "    filtered_distances = np.array(distances)\n",
        "\n",
        "    print(f\"Found {len(nu_trees)} trees with distance < {distance_threshold}\")\n",
        "\n",
        "    return mu_tree, mu_path, nu_trees, nu_paths, filtered_distances\n",
        "\n",
        "def distributionally_robust_loss(batch_weights=None):\n",
        "    \"\"\"\n",
        "    Create a custom loss function that emphasizes the worst-case examples.\n",
        "\n",
        "    Args:\n",
        "        batch_weights: Optional weights to apply to each example in the batch.\n",
        "\n",
        "    Returns:\n",
        "        A loss function that can be used by the Keras model.\n",
        "    \"\"\"\n",
        "    def dro_loss(y_true, y_pred):\n",
        "        # Calculate squared error for each sample\n",
        "        squared_errors = tf.reduce_mean(tf.square(y_pred - y_true), axis=1)\n",
        "        return tf.reduce_max(squared_errors)\n",
        "\n",
        "    return dro_loss\n",
        "\n",
        "def train_neural_network_dro(mu_path, nu_paths, epochs=50, batch_size=16, patience=10):\n",
        "    \"\"\"Train a neural network using the DRO approach.\"\"\"\n",
        "    mu_values = path_to_values_vector(mu_path)\n",
        "    mu_labels = path_to_binary_label(mu_path)\n",
        "\n",
        "    nu_values = np.array([path_to_values_vector(path) for path in nu_paths])\n",
        "    nu_labels = np.array([path_to_binary_label(path) for path in nu_paths])\n",
        "\n",
        "    model = create_neural_network(input_size=10)\n",
        "\n",
        "    # Training history to see how max_error changes\n",
        "    max_error_history = []\n",
        "\n",
        "    # Early stopping\n",
        "    best_max_error = float('inf')\n",
        "    best_model_weights = None\n",
        "    patience_counter = 0\n",
        "\n",
        "    print(f\"Training neural network with {len(nu_paths)} nu paths using DRO approach...\")\n",
        "\n",
        "    # Combined dataset for evaluation\n",
        "    all_values = np.vstack([nu_values])\n",
        "    all_labels = np.vstack([nu_labels])\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # identify the worst-case distribution (nu path with max error)\n",
        "        all_error_rates = []\n",
        "\n",
        "        for i, (values, labels) in enumerate(zip(nu_values, nu_labels)):\n",
        "            # Predict\n",
        "            predictions = model.predict(np.array([values]), verbose=0)[0]\n",
        "\n",
        "            # Convert to binary predictions (1 if > 1, else -1)\n",
        "            binary_predictions = np.where(predictions > 1, 1, -1)\n",
        "\n",
        "            # Calculate error rate\n",
        "            error_rate = 1.0 - np.mean(binary_predictions == labels)\n",
        "            all_error_rates.append(error_rate)\n",
        "\n",
        "        # Find the worst-case path (highest error)\n",
        "        if all_error_rates:\n",
        "            max_error_idx = np.argmax(all_error_rates)\n",
        "            max_error = all_error_rates[max_error_idx]\n",
        "            max_error_history.append(max_error)\n",
        "\n",
        "        # Early stopping check\n",
        "        if max_error < best_max_error:\n",
        "            best_max_error = max_error\n",
        "            best_model_weights = model.get_weights()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}. No improvement for {patience} epochs.\")\n",
        "            # Restore best weights\n",
        "            model.set_weights(best_model_weights)\n",
        "            break\n",
        "\n",
        "        # emphasize the worst-case paths\n",
        "        sample_weights = np.ones(len(nu_paths))\n",
        "        sample_weights[max_error_idx] = 5.0  # Give 5x weight to the worst-case path\n",
        "\n",
        "        # Create a custom loss that focuses on the worst-case\n",
        "        custom_loss = distributionally_robust_loss()\n",
        "        model.compile(optimizer=Adam(learning_rate=0.001), loss=custom_loss)\n",
        "\n",
        "        # Train with custom weights\n",
        "        model.fit(\n",
        "            all_values,\n",
        "            all_labels,\n",
        "            sample_weight=sample_weights,\n",
        "            epochs=1,\n",
        "            verbose=0,\n",
        "            batch_size=batch_size\n",
        "        )\n",
        "\n",
        "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs}: Worst-case error rate = {max_error:.4f} (path {max_error_idx+1}), Best = {best_max_error:.4f}\")\n",
        "\n",
        "    # Make sure we're using the best model weights\n",
        "    if best_model_weights is not None:\n",
        "        model.set_weights(best_model_weights)\n",
        "\n",
        "    return model\n",
        "\n",
        "def main_algorithm1():\n",
        "    # Parameters\n",
        "    num_nu_trees = 200  # Number of trees\n",
        "    num_layers = 10     # Number of layers in each tree\n",
        "    max_children = 2    # Maximum number of children per node\n",
        "    distance_threshold = 0.08  # Maximum distance for filtering\n",
        "    early_stopping_patience = 10  # Number of epochs with no improvement before stopping\n",
        "\n",
        "    random.seed(42)\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    # Generate reference tree (mu) and sample a path\n",
        "    print(\"Generating mu tree...\")\n",
        "    mu_tree = generate_random_tree(num_layers, max_children)\n",
        "    mu_path = sample_path_from_tree(mu_tree, num_layers)\n",
        "\n",
        "    # Generate and filter nu trees\n",
        "    print(\"\\nGenerating and filtering nu trees for DRO algorithm...\")\n",
        "    _, _, nu_trees, nu_paths, distances = generate_and_filter_trees(\n",
        "        num_nu_trees=num_nu_trees,\n",
        "        num_layers=num_layers,\n",
        "        max_children=max_children,\n",
        "        distance_threshold=distance_threshold\n",
        "    )\n",
        "\n",
        "    if len(nu_trees) < 10:\n",
        "        print(\"Not enough trees found within threshold.\")\n",
        "        distance_threshold *= 2\n",
        "        _, _, nu_trees, nu_paths, distances = generate_and_filter_trees(\n",
        "            num_nu_trees=num_nu_trees,\n",
        "            num_layers=num_layers,\n",
        "            max_children=max_children,\n",
        "            distance_threshold=distance_threshold\n",
        "        )\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    n_total = len(nu_trees)\n",
        "    n_test = max(1, int(n_total * 0.1))  # 10% for testing\n",
        "    n_train = n_total - n_test  # Rest for training\n",
        "\n",
        "    # Create indices for splitting\n",
        "    indices = np.arange(n_total)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    test_indices = indices[:n_test]\n",
        "    train_indices = indices[n_test:]\n",
        "\n",
        "    # Extract datasets\n",
        "    train_nu_paths = [nu_paths[i] for i in train_indices]\n",
        "    test_nu_paths = [nu_paths[i] for i in test_indices]\n",
        "\n",
        "    train_distances = distances[train_indices]\n",
        "    test_distances = distances[test_indices]\n",
        "\n",
        "    print(f\"\\nData split:\")\n",
        "    print(f\"Training: {len(train_nu_paths)} paths\")\n",
        "    print(f\"Testing: {len(test_nu_paths)} paths\")\n",
        "\n",
        "    # ===== ALGORITHM 1: Train with DRO approach =====\n",
        "    print(\"\\n===== ALGORITHM 1: Distributionally Robust Optimization =====\")\n",
        "    dro_model = train_neural_network_dro(\n",
        "        mu_path, train_nu_paths,\n",
        "        epochs=50, batch_size=16, patience=early_stopping_patience\n",
        "    )\n",
        "\n",
        "    # Evaluate on test set\n",
        "    print(\"\\n===== EVALUATING DRO MODEL ON TEST SET =====\")\n",
        "    test_values = np.array([path_to_values_vector(path) for path in test_nu_paths])\n",
        "    test_labels = np.array([path_to_binary_label(path) for path in test_nu_paths])\n",
        "\n",
        "    # Get predictions\n",
        "    test_predictions = dro_model.predict(test_values, verbose=0)\n",
        "    test_binary_predictions = np.where(test_predictions > 1, 1, -1)\n",
        "\n",
        "    # Calculate accuracy and error rates\n",
        "    accuracy_rates = []\n",
        "    error_rates = []\n",
        "\n",
        "    for i in range(len(test_nu_paths)):\n",
        "        accuracy = np.mean(test_binary_predictions[i] == test_labels[i])\n",
        "        error = 1.0 - accuracy\n",
        "\n",
        "        accuracy_rates.append(accuracy)\n",
        "        error_rates.append(error)\n",
        "\n",
        "        print(f\"Test path {i+1}: Accuracy = {accuracy:.4f}, Error = {error:.4f}, Distance = {test_distances[i]:.4f}\")\n",
        "\n",
        "    mean_accuracy = np.mean(accuracy_rates)\n",
        "    mean_error = np.mean(error_rates)\n",
        "    max_error = np.max(error_rates)\n",
        "    max_error_idx = np.argmax(error_rates)\n",
        "\n",
        "    print(f\"\\nTest set results:\")\n",
        "    print(f\"Mean accuracy: {mean_accuracy:.4f}\")\n",
        "    print(f\"Mean error rate: {mean_error:.4f}\")\n",
        "    print(f\"Maximum error rate: {max_error:.4f} (path {max_error_idx+1})\")\n",
        "\n",
        "    # Plot error rates\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(range(1, len(error_rates) + 1), error_rates)\n",
        "    plt.xlabel('Test Path Index')\n",
        "    plt.ylabel('Error Rate')\n",
        "    plt.title('DRO Model Error Rates on Test Set')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.savefig('dro_test_error_rates.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Plot relationship between distance and error\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(test_distances, error_rates, alpha=0.7)\n",
        "    plt.xlabel('Nested Wasserstein Distance')\n",
        "    plt.ylabel('Error Rate')\n",
        "    plt.title('Relationship Between Distance and Error Rate (DRO Model)')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.savefig('dro_distance_vs_error.png')\n",
        "    plt.close()\n",
        "\n",
        "    return {\n",
        "        'mu_path': mu_path,\n",
        "        'train_paths': train_nu_paths,\n",
        "        'test_paths': test_nu_paths,\n",
        "        'test_values': test_values,\n",
        "        'test_labels': test_labels,\n",
        "        'test_distances': test_distances,\n",
        "        'dro_model': dro_model,\n",
        "        'dro_predictions': test_predictions,\n",
        "        'dro_binary_predictions': test_binary_predictions,\n",
        "        'accuracy_rates': accuracy_rates,\n",
        "        'error_rates': error_rates,\n",
        "        'mean_accuracy': mean_accuracy,\n",
        "        'mean_error': mean_error,\n",
        "        'max_error': max_error\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = main_algorithm1()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8xKdcMQqlXf",
        "outputId": "8f3f787f-b8cb-4b09-f427-5fae9ffa1f1f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating mu tree...\n",
            "\n",
            "Generating and filtering nu trees for DRO algorithm...\n",
            "Generating and filtering 200 nu trees...\n",
            "Processed 20/200 trees, found 13 within threshold\n",
            "Processed 40/200 trees, found 29 within threshold\n",
            "Processed 60/200 trees, found 48 within threshold\n",
            "Processed 80/200 trees, found 63 within threshold\n",
            "Processed 100/200 trees, found 81 within threshold\n",
            "Processed 120/200 trees, found 100 within threshold\n",
            "Processed 140/200 trees, found 115 within threshold\n",
            "Processed 160/200 trees, found 133 within threshold\n",
            "Processed 180/200 trees, found 150 within threshold\n",
            "Processed 200/200 trees, found 169 within threshold\n",
            "Found 169 trees with distance < 0.08\n",
            "\n",
            "Data split:\n",
            "Training: 153 paths\n",
            "Testing: 16 paths\n",
            "\n",
            "===== ALGORITHM 1: Distributionally Robust Optimization =====\n",
            "Training neural network with 153 nu paths using DRO approach...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50: Worst-case error rate = 0.8000 (path 34), Best = 0.8000\n",
            "Epoch 5/50: Worst-case error rate = 0.8000 (path 34), Best = 0.8000\n",
            "Epoch 10/50: Worst-case error rate = 0.8000 (path 34), Best = 0.8000\n",
            "Early stopping at epoch 11. No improvement for 10 epochs.\n",
            "\n",
            "===== EVALUATING DRO MODEL ON TEST SET =====\n",
            "Test path 1: Accuracy = 0.7000, Error = 0.3000, Distance = 0.0725\n",
            "Test path 2: Accuracy = 0.4000, Error = 0.6000, Distance = 0.0698\n",
            "Test path 3: Accuracy = 0.5000, Error = 0.5000, Distance = 0.0656\n",
            "Test path 4: Accuracy = 0.7000, Error = 0.3000, Distance = 0.0768\n",
            "Test path 5: Accuracy = 0.5000, Error = 0.5000, Distance = 0.0724\n",
            "Test path 6: Accuracy = 0.3000, Error = 0.7000, Distance = 0.0690\n",
            "Test path 7: Accuracy = 0.6000, Error = 0.4000, Distance = 0.0770\n",
            "Test path 8: Accuracy = 0.3000, Error = 0.7000, Distance = 0.0662\n",
            "Test path 9: Accuracy = 0.8000, Error = 0.2000, Distance = 0.0800\n",
            "Test path 10: Accuracy = 0.4000, Error = 0.6000, Distance = 0.0711\n",
            "Test path 11: Accuracy = 0.8000, Error = 0.2000, Distance = 0.0780\n",
            "Test path 12: Accuracy = 0.6000, Error = 0.4000, Distance = 0.0753\n",
            "Test path 13: Accuracy = 0.6000, Error = 0.4000, Distance = 0.0787\n",
            "Test path 14: Accuracy = 0.3000, Error = 0.7000, Distance = 0.0721\n",
            "Test path 15: Accuracy = 0.5000, Error = 0.5000, Distance = 0.0748\n",
            "Test path 16: Accuracy = 0.6000, Error = 0.4000, Distance = 0.0766\n",
            "\n",
            "Test set results:\n",
            "Mean accuracy: 0.5375\n",
            "Mean error rate: 0.4625\n",
            "Maximum error rate: 0.7000 (path 6)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNH7kAuJzlPF1UIz677j+gZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}